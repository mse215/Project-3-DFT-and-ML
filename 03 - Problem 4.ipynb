{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Using Machine Learning to Predict Bulk Modulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTICE TO BINDER USERS: YOUR NOTEBOOK PROGRESS WILL NOT BE SAVED IF YOU CLOSE THIS WINDOW OR LEAVE IT INACTIVE FOR TOO LONG.\n",
    "\n",
    "PLEASE DOWNLOAD YOUR NOTEBOOKS AND FILES REGULARLY OR DOWNLOAD THIS REPO AND RUN OFFLINE ON YOUR MACHINE. See \"running_offline.md\" for more info.\n",
    "\n",
    "\n",
    "This notebook is based on a Matminer example notebook written by Anubhav Jain, which can be found [here](https://github.com/hackingmaterials/matminer_examples/blob/master/matminer_examples/machine_learning-nb/bulk_modulus.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Construct a Linear Regression Model\n",
    "\n",
    "Linear regression, also known as ordinary least squares regression, is a fundamental technique for learning a linear model from a set of data. Because it is so foundational, most machine learning libraries have some kind of function or object that performs linear regression pre-defined. In this notebook, we'll be using [scikit-learn's](https://scikit-learn.org/stable/index.html) linear regression model and evaluating our performance with the \"root mean squared error\" or RMSE (the square root of the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)). Please define a function for RMSE below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_predicted):\n",
    "    # YOUR CODE HERE\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import our data. The provided training data is already in a tabular form (.csv stands for \"comma separated values\" and is a tabular file type), so we can readily use a [pandas dataframe](https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python) to load the data from \"ml_training_data.csv' and make it easier to manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "df = read_csv(\"ml_training_data.csv\")\n",
    "# Drop the unnecessesary index column\n",
    "df = df.drop(\"Unnamed: 0\", 1)\n",
    "# Show first 10 rows of our data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to separate our training data into what we're trying to predict (K_VRH) and the features we're trying to use for our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a numpy array from a the \"K_VRH\" column of the dataframe\n",
    "y = df[\"K_VRH\"].to_numpy()\n",
    "\n",
    "# Creates a numpy array from the dataframe after removing unwanted columns (we don't want to predict K_VRH from K_VRH.)\n",
    "X = df.drop(labels=[\"material_id\", \"K_VRH\", \"kpoint_density\"], axis = 1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We're ready to fit our linear model to this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "# get fit statistics\n",
    "print('training R2 = ' + str(round(lr.score(X, y), 3)))\n",
    "print('training RMSE = %.3f GPa' % rmse(y_true=y, y_predicted=lr.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Use 10-fold cross validation (90% training, 10% test)\n",
    "crossvalidation = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "scores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=1)\n",
    "rmse_scores = [np.sqrt(abs(s)) for s in scores]\n",
    "r2_scores = cross_val_score(lr, X, y, scoring='r2', cv=crossvalidation, n_jobs=1)\n",
    "\n",
    "print('Cross-validation results:')\n",
    "print('Folds: %i, mean R2: %.3f' % (len(scores), np.mean(np.abs(r2_scores))))\n",
    "print('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a *figrecipe* from matminer to plot how our linear model's predictions compare to the DFT calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matminer.figrecipes.plot import PlotlyFig\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pf = PlotlyFig(x_title='DFT (MP) bulk modulus (GPa)',\n",
    "               y_title='Predicted bulk modulus (GPa)',\n",
    "               title='Linear regression',\n",
    "               mode='notebook',\n",
    "               filename=\"lr_regression.html\")\n",
    "\n",
    "pf.xy(xy_pairs=[(y, cross_val_predict(lr, X, y, cv=crossvalidation)), ([0, max(y)], [0, max(y)])], \n",
    "      labels=df['material_id'], \n",
    "      modes=['markers', 'lines'],\n",
    "      lines=[{}, {'color': 'black', 'dash': 'dash'}], \n",
    "      showlegends=False\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the DFT calculated values of K_VRH from the materials project (you should compare to your calculations from Problem 2 as well for your report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_df = read_csv(\"Si_features.csv\")\n",
    "Si =  Si_df.drop(['material_id', 'material_name'], axis=1)[df.drop(['material_id', 'kpoint_density', 'K_VRH'], axis=1).columns.values].to_numpy()\n",
    "predictions = lr.predict(Si)\n",
    "print(\"Diamond Cubic:\")\n",
    "print(\"\\tPrediction: {0:.0f} GPa\".format(predictions[0]))\n",
    "print(\"\\tMP DFT: {0:.0f} GPa\".format(83.0))\n",
    "print(\"Beta Tin:\")\n",
    "print(\"\\tPrediction: {0:.0f} GPa\".format(predictions[1]))\n",
    "print(\"\\tMP DFT: {0:.0f} GPa\".format(108.0))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While simple and easy to interpret, linear regressions can be quite useful. Let's try a slightly more advanced form of machine learning model, a random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=1)\n",
    "\n",
    "rf.fit(X, y)\n",
    "print('training R2 = ' + str(round(rf.score(X, y), 3)))\n",
    "print('training RMSE = %.3f' % np.sqrt(mean_squared_error(y_true=y, y_pred=rf.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Use 10-fold cross validation (90% training, 10% test)\n",
    "crossvalidation = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "scores = cross_val_score(rf, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=1)\n",
    "rmse_scores = [np.sqrt(abs(s)) for s in scores]\n",
    "r2_scores = cross_val_score(rf, X, y, scoring='r2', cv=crossvalidation, n_jobs=1)\n",
    "\n",
    "print('Cross-validation results:')\n",
    "print('Folds: %i, mean R2: %.3f' % (len(scores), np.mean(np.abs(r2_scores))))\n",
    "print('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matminer.figrecipes.plot import PlotlyFig\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "pf = PlotlyFig(x_title='DFT (MP) bulk modulus (GPa)',\n",
    "               y_title='Predicted bulk modulus (GPa)',\n",
    "               title='Random Forest',\n",
    "               mode='notebook',\n",
    "               filename=\"random_forest.html\")\n",
    "\n",
    "pf.xy(xy_pairs=[(y, cross_val_predict(rf, X, y, cv=crossvalidation)), ([0, max(y)], [0, max(y)])], \n",
    "      labels=df['material_id'], \n",
    "      modes=['markers', 'lines'],\n",
    "      lines=[{}, {'color': 'black', 'dash': 'dash'}], \n",
    "      showlegends=False\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the DFT calculated values of K_VRH from the materials project (you should compare to your calculations from Problem 2 as well for your report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_df = read_csv(\"Si_features.csv\")\n",
    "Si =  Si_df.drop(['material_id', 'material_name'], axis=1)[df.drop(['material_id', 'kpoint_density', 'K_VRH'], axis=1).columns.values].to_numpy()\n",
    "predictions = rf.predict(Si)\n",
    "print(\"Diamond Cubic:\")\n",
    "print(\"\\tPrediction: {0:.0f} GPa\".format(predictions[0]))\n",
    "print(\"\\tMP DFT: {0:.0f} GPa\".format(83.0))\n",
    "print(\"Beta Tin:\")\n",
    "print(\"\\tPrediction: {0:.0f} GPa\".format(predictions[1]))\n",
    "print(\"\\tMP DFT: {0:.0f} GPa\".format(108.0))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to programatically get CPU time used during DFT Calculations\n",
    "\n",
    "Rather than going through the OUTCARs by hand to get the calculation statistics, we can use pymatgen's Outcar object to help speed up the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.io.vasp import Outcar\n",
    "\n",
    "outcar_file = # YOUR CODE HERE\n",
    "\n",
    "time = Outcar(outcar_file).run_stats[\"Total CPU time used (sec)\"]\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_directory = # YOUR CODE HERE\n",
    "total_time = 0\n",
    "for subpath, subdirs, subfiles in os.walk(my_directory):\n",
    "    for subdir in subdirs:\n",
    "        outcar_file = os.path.join(subpath, subdir) + \"/OUTCAR\"\n",
    "        total_time += # YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mse215]",
   "language": "python",
   "name": "conda-env-mse215-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
